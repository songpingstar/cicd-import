{
    "instance_id": "Parsl__parsl-1175",
    "patch": "diff --git a/parsl/launchers/__init__.py b/parsl/launchers/__init__.py\nindex 8321ba5912..29fa2ee06e 100644\n--- a/parsl/launchers/__init__.py\n+++ b/parsl/launchers/__init__.py\n@@ -1,13 +1,13 @@\n from parsl.launchers.launchers import SimpleLauncher, SingleNodeLauncher, \\\n     SrunLauncher, AprunLauncher, SrunMPILauncher, \\\n-    GnuParallelLauncher, MpiExecLauncher, MpiRunLauncher\n+    GnuParallelLauncher, MpiExecLauncher, MpiRunLauncher, JsrunLauncher\n \n __all__ = ['SimpleLauncher',\n            'SingleNodeLauncher',\n            'SrunLauncher',\n            'AprunLauncher',\n            'SrunMPILauncher',\n-           'AprunLauncher',\n+           'JsrunLauncher',\n            'GnuParallelLauncher',\n            'MpiExecLauncher',\n            'MpiRunLauncher']\ndiff --git a/parsl/launchers/launchers.py b/parsl/launchers/launchers.py\nindex 64a6a1a641..b6da3e5359 100644\n--- a/parsl/launchers/launchers.py\n+++ b/parsl/launchers/launchers.py\n@@ -357,6 +357,50 @@ def __call__(self, command, tasks_per_node, nodes_per_block):\n         return x\n \n \n+class JsrunLauncher(Launcher):\n+    \"\"\"  Worker launcher that wraps the user's command with the Jsrun launch framework\n+    to launch multiple cmd invocations in parallel on a single job allocation\n+\n+    \"\"\"\n+    def __init__(self, overrides=''):\n+        \"\"\"\n+        Parameters\n+        ----------\n+\n+        overrides: str\n+             This string will be passed to the JSrun launcher. Default: ''\n+        \"\"\"\n+        self.overrides = overrides\n+\n+    def __call__(self, command, tasks_per_node, nodes_per_block):\n+        \"\"\"\n+        Args:\n+        - command (string): The command string to be launched\n+        - tasks_per_node (int) : Workers to launch per node\n+        - nodes_per_block (int) : Number of nodes in a block\n+\n+        \"\"\"\n+\n+        tasks_per_block = tasks_per_node * nodes_per_block\n+        x = '''\n+WORKERCOUNT={1}\n+\n+cat << JSRUN_EOF > cmd_$JOBNAME.sh\n+{0}\n+JSRUN_EOF\n+chmod a+x cmd_$JOBNAME.sh\n+\n+jsrun -n {tasks_per_block} -r {tasks_per_node} {overrides} /bin/bash cmd_$JOBNAME.sh &\n+wait\n+\n+echo \"Done\"\n+'''.format(command, tasks_per_block,\n+           tasks_per_block=tasks_per_block,\n+           tasks_per_node=tasks_per_node,\n+           overrides=self.overrides)\n+        return x\n+\n+\n if __name__ == '__main__':\n \n     s = SingleNodeLauncher()\ndiff --git a/parsl/providers/__init__.py b/parsl/providers/__init__.py\nindex e6b30a70b0..0fb1a1477f 100644\n--- a/parsl/providers/__init__.py\n+++ b/parsl/providers/__init__.py\n@@ -9,6 +9,7 @@\n from parsl.providers.slurm.slurm import SlurmProvider\n from parsl.providers.torque.torque import TorqueProvider\n from parsl.providers.pbspro.pbspro import PBSProProvider\n+from parsl.providers.lsf.lsf import LSFProvider\n \n # Cloud Providers\n from parsl.providers.aws.aws import AWSProvider\n@@ -25,6 +26,7 @@\n            'GridEngineProvider',\n            'SlurmProvider',\n            'TorqueProvider',\n+           'LSFProvider',\n            'PBSProProvider',\n            'AWSProvider',\n            'GoogleCloudProvider',\ndiff --git a/parsl/providers/lsf/__init__.py b/parsl/providers/lsf/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/parsl/providers/lsf/lsf.py b/parsl/providers/lsf/lsf.py\nnew file mode 100644\nindex 0000000000..b9a11e1599\n--- /dev/null\n+++ b/parsl/providers/lsf/lsf.py\n@@ -0,0 +1,219 @@\n+import os\n+import time\n+import logging\n+\n+from parsl.channels import LocalChannel\n+from parsl.launchers import SingleNodeLauncher\n+from parsl.providers.cluster_provider import ClusterProvider\n+from parsl.providers.lsf.template import template_string\n+from parsl.utils import RepresentationMixin, wtime_to_minutes\n+\n+logger = logging.getLogger(__name__)\n+\n+translate_table = {\n+    'PEND': 'PENDING',\n+    'RUN': 'RUNNING',\n+    'DONE': 'COMPLETED',\n+    'EXIT': 'FAILED',  # (failed),\n+    'PSUSP': 'CANCELLED',\n+    'USUSP': 'CANCELLED',\n+    'SSUSP': 'CANCELLED',\n+}\n+\n+\n+class LSFProvider(ClusterProvider, RepresentationMixin):\n+    \"\"\"LSF Execution Provider\n+\n+    This provider uses sbatch to submit, squeue for status and scancel to cancel\n+    jobs. The sbatch script to be used is created from a template file in this\n+    same module.\n+\n+    Parameters\n+    ----------\n+    channel : Channel\n+        Channel for accessing this provider. Possible channels include\n+        :class:`~parsl.channels.LocalChannel` (the default),\n+        :class:`~parsl.channels.SSHChannel`, or\n+        :class:`~parsl.channels.SSHInteractiveLoginChannel`.\n+    nodes_per_block : int\n+        Nodes to provision per block.\n+    init_blocks : int\n+        Number of blocks to request at the start of the run.\n+    min_blocks : int\n+        Minimum number of blocks to maintain.\n+    max_blocks : int\n+        Maximum number of blocks to maintain.\n+    parallelism : float\n+        Ratio of provisioned task slots to active tasks. A parallelism value of 1 represents aggressive\n+        scaling where as many resources as possible are used; parallelism close to 0 represents\n+        the opposite situation in which as few resources as possible (i.e., min_blocks) are used.\n+    walltime : str\n+        Walltime requested per block in HH:MM:SS.\n+    project : str\n+        Project to which the resources must be charged\n+    scheduler_options : str\n+        String to prepend to the #SBATCH blocks in the submit script to the scheduler.\n+    worker_init : str\n+        Command to be run before starting a worker, such as 'module load Anaconda; source activate env'.\n+    cmd_timeout : int\n+        Seconds after which requests to the scheduler will timeout. Default: 120s\n+    launcher : Launcher\n+        Launcher for this provider. Possible launchers include\n+        :class:`~parsl.launchers.SingleNodeLauncher` (the default),\n+        :class:`~parsl.launchers.SrunLauncher`, or\n+        :class:`~parsl.launchers.AprunLauncher`\n+     move_files : Optional[Bool]: should files be moved? by default, Parsl will try to move files.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 channel=LocalChannel(),\n+                 nodes_per_block=1,\n+                 init_blocks=1,\n+                 min_blocks=0,\n+                 max_blocks=10,\n+                 parallelism=1,\n+                 walltime=\"00:10:00\",\n+                 scheduler_options='',\n+                 worker_init='',\n+                 project=None,\n+                 cmd_timeout=120,\n+                 move_files=True,\n+                 launcher=SingleNodeLauncher()):\n+        label = 'LSF'\n+        super().__init__(label,\n+                         channel,\n+                         nodes_per_block,\n+                         init_blocks,\n+                         min_blocks,\n+                         max_blocks,\n+                         parallelism,\n+                         walltime,\n+                         cmd_timeout=cmd_timeout,\n+                         launcher=launcher)\n+\n+        self.project = project\n+        self.move_files = move_files\n+        self.scheduler_options = scheduler_options\n+        self.worker_init = worker_init\n+\n+    def _status(self):\n+        ''' Internal: Do not call. Returns the status list for a list of job_ids\n+\n+        Args:\n+              self\n+\n+        Returns:\n+              [status...] : Status list of all jobs\n+        '''\n+        job_id_list = ','.join(self.resources.keys())\n+        cmd = \"bjobs {0}\".format(job_id_list)\n+\n+        retcode, stdout, stderr = super().execute_wait(cmd)\n+        # Execute_wait failed. Do no update\n+        if retcode != 0:\n+            logger.debug(\"Updating job status from {} failed with return code {}\".format(self.label,\n+                                                                                         retcode))\n+            return\n+\n+        jobs_missing = list(self.resources.keys())\n+        for line in stdout.split('\\n'):\n+            parts = line.split()\n+            if parts and parts[0] != 'JOBID':\n+                job_id = parts[0]\n+                status = translate_table.get(parts[2], 'UNKNOWN')\n+                self.resources[job_id]['status'] = status\n+                jobs_missing.remove(job_id)\n+\n+        # squeue does not report on jobs that are not running. So we are filling in the\n+        # blanks for missing jobs, we might lose some information about why the jobs failed.\n+        for missing_job in jobs_missing:\n+            if self.resources[missing_job]['status'] in ['PENDING', 'RUNNING']:\n+                self.resources[missing_job]['status'] = 'COMPLETED'\n+\n+    def submit(self, command, tasks_per_node, job_name=\"parsl.auto\"):\n+        \"\"\"Submit the command as an LSF job.\n+\n+        Parameters\n+        ----------\n+        command : str\n+            Command to be made on the remote side.\n+        tasks_per_node : int\n+            Command invocations to be launched per node\n+        job_name : str\n+            Name for the job (must be unique).\n+        Returns\n+        -------\n+        None or str\n+            If at capacity, returns None; otherwise, a string identifier for the job\n+        \"\"\"\n+\n+        if self.provisioned_blocks >= self.max_blocks:\n+            logger.warn(\"LSF provider '{}' is at capacity (no more blocks will be added)\".format(self.label))\n+            return None\n+\n+        job_name = \"{0}.{1}\".format(job_name, time.time())\n+\n+        script_path = \"{0}/{1}.submit\".format(self.script_dir, job_name)\n+        script_path = os.path.abspath(script_path)\n+\n+        logger.debug(\"Requesting one block with {} nodes\".format(self.nodes_per_block))\n+\n+        job_config = {}\n+        job_config[\"submit_script_dir\"] = self.channel.script_dir\n+        job_config[\"nodes\"] = self.nodes_per_block\n+        job_config[\"tasks_per_node\"] = tasks_per_node\n+        job_config[\"walltime\"] = wtime_to_minutes(self.walltime)\n+        job_config[\"scheduler_options\"] = self.scheduler_options\n+        job_config[\"worker_init\"] = self.worker_init\n+        job_config[\"project\"] = self.project\n+        job_config[\"user_script\"] = command\n+\n+        # Wrap the command\n+        job_config[\"user_script\"] = self.launcher(command,\n+                                                  tasks_per_node,\n+                                                  self.nodes_per_block)\n+\n+        logger.debug(\"Writing submit script\")\n+        self._write_submit_script(template_string, script_path, job_name, job_config)\n+\n+        if self.move_files:\n+            logger.debug(\"moving files\")\n+            channel_script_path = self.channel.push_file(script_path, self.channel.script_dir)\n+        else:\n+            logger.debug(\"not moving files\")\n+            channel_script_path = script_path\n+\n+        retcode, stdout, stderr = super().execute_wait(\"bsub {0}\".format(channel_script_path))\n+\n+        job_id = None\n+        if retcode == 0:\n+            for line in stdout.split('\\n'):\n+                if line.lower().startswith(\"job\") and \"is submitted to\" in line.lower():\n+                    job_id = line.split()[1].strip('<>')\n+                    self.resources[job_id] = {'job_id': job_id, 'status': 'PENDING'}\n+        else:\n+            logger.warning(\"Submission of command to scale_out failed\")\n+            logger.error(\"Retcode:%s STDOUT:%s STDERR:%s\", retcode, stdout.strip(), stderr.strip())\n+        return job_id\n+\n+    def cancel(self, job_ids):\n+        ''' Cancels the jobs specified by a list of job ids\n+\n+        Args:\n+        job_ids : [<job_id> ...]\n+\n+        Returns :\n+        [True/False...] : If the cancel operation fails the entire list will be False.\n+        '''\n+\n+        job_id_list = ' '.join(job_ids)\n+        retcode, stdout, stderr = super().execute_wait(\"bkill {0}\".format(job_id_list))\n+        rets = None\n+        if retcode == 0:\n+            for jid in job_ids:\n+                self.resources[jid]['status'] = translate_table['USUSP']  # Job suspended by user/admin\n+            rets = [True for i in job_ids]\n+        else:\n+            rets = [False for i in job_ids]\n+\n+        return rets\ndiff --git a/parsl/providers/lsf/template.py b/parsl/providers/lsf/template.py\nnew file mode 100644\nindex 0000000000..d7109e6b44\n--- /dev/null\n+++ b/parsl/providers/lsf/template.py\n@@ -0,0 +1,16 @@\n+template_string = '''#!/bin/bash\n+\n+#BSUB -P ${project}\n+#BSUB -W ${walltime}\n+#BSUB -nnodes ${nodes}\n+#BSUB -J ${jobname}\n+#BSUB -o ${submit_script_dir}/${jobname}.submit.stdout\n+#BSUB -e ${submit_script_dir}/${jobname}.submit.stderr\n+${scheduler_options}\n+\n+${worker_init}\n+\n+export JOBNAME=\"${jobname}\"\n+\n+$user_script\n+'''\n",
    "repo": "Parsl/parsl",
    "base_commit": "fe6d47e0774935e82e14f4cb8123c795f24c627f",
    "hints_text": "No, I dont know of any LSF machines.  NCAR Yellowstone was LSF but was\ndecommissioned over a year ago.\n\n\nOn Wed, Mar 6, 2019 at 1:26 PM Yadu Nand Babuji <notifications@github.com>\nwrote:\n\n> We do not currently have a provider that supports LSF clusters. So far\n> we've not had access to cluster that uses this batch system, but a\n> potential user (@erykoff <https://github.com/erykoff>) from SLAC offered\n> to help in this regard.\n>\n> I imagine most of the effort in adding a new provider would be in testing\n> rather than development given the several providers (eg, slurm) that can be\n> used as templates.\n>\n> @mjwilde <https://github.com/mjwilde>, would you know any LSF clusters\n> that we could get access to?\n>\n> â€”\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Parsl/parsl/issues/806>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABqb2H7f5ggtRxhYpmDyJPJEFGLp4S4xks5vUBZpgaJpZM4bho1h>\n> .\n>\n\nSummit is LSF, I could try it there.\n+1 for a LSF provider, we have several researchers who need this and are limited to something like dask-jobqueue at the moment.\nI can look at this next week...",
    "created_at": "2019-08-05T17:56:44Z",
    "test_patch": "diff --git a/parsl/tests/configs/summit_local_htex.py b/parsl/tests/configs/summit_local_htex.py\nnew file mode 100644\nindex 0000000000..38a0a953b0\n--- /dev/null\n+++ b/parsl/tests/configs/summit_local_htex.py\n@@ -0,0 +1,29 @@\n+from parsl.config import Config\n+from parsl.executors import HighThroughputExecutor\n+\n+from parsl.launchers import JsrunLauncher\n+from parsl.providers import LSFProvider\n+\n+from parsl.addresses import address_by_interface\n+\n+config = Config(\n+    executors=[\n+        HighThroughputExecutor(\n+            label='Summit_HTEX',\n+            working_dir='/gpfs/alpine/scratch/yadunan/gen011/',\n+            address=address_by_interface('ib0'),  # This assumes Parsl is running on login node\n+            worker_port_range=(50000, 55000),\n+            provider=LSFProvider(\n+                launcher=JsrunLauncher(),\n+                walltime=\"00:10:00\",\n+                nodes_per_block=2,\n+                init_blocks=1,\n+                max_blocks=1,\n+                worker_init=\"source ~/setup.sh\",\n+                project='GEN011WORKFLOW',\n+                cmd_timeout=60\n+            ),\n+        )\n+\n+    ],\n+)\n",
    "problem_statement": "LSF Provider\nWe do not currently have a provider that supports LSF clusters. So far we've not had access to cluster that uses this batch system, but a potential user (@erykoff) from SLAC offered to help in this regard.\r\n\r\nI imagine most of the effort in adding a new provider would be in testing rather than development given the several providers (eg, slurm) that can be used as templates.\r\n\r\n@mjwilde, would you know any LSF clusters that we could get access to? \n",
    "environment_setup_commit": "fe6d47e0774935e82e14f4cb8123c795f24c627f",
    "pr_url": "https://github.com/Parsl/parsl/pull/1175",
    "issue_url": "https://github.com/Parsl/parsl/issues/806",
    "issue_numbers": [
        "806"
    ]
}