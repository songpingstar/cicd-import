{
    "instance_id": "llamastack__llama-stack-3371",
    "patch": "diff --git a/docs/_static/llama-stack-spec.html b/docs/_static/llama-stack-spec.html\nindex a036e5dc07..9ddb070d77 100644\n--- a/docs/_static/llama-stack-spec.html\n+++ b/docs/_static/llama-stack-spec.html\n@@ -1380,6 +1380,40 @@\n                         }\n                     }\n                 ]\n+            },\n+            \"delete\": {\n+                \"responses\": {\n+                    \"200\": {\n+                        \"description\": \"OK\"\n+                    },\n+                    \"400\": {\n+                        \"$ref\": \"#/components/responses/BadRequest400\"\n+                    },\n+                    \"429\": {\n+                        \"$ref\": \"#/components/responses/TooManyRequests429\"\n+                    },\n+                    \"500\": {\n+                        \"$ref\": \"#/components/responses/InternalServerError500\"\n+                    },\n+                    \"default\": {\n+                        \"$ref\": \"#/components/responses/DefaultError\"\n+                    }\n+                },\n+                \"tags\": [\n+                    \"Benchmarks\"\n+                ],\n+                \"description\": \"Unregister a benchmark.\",\n+                \"parameters\": [\n+                    {\n+                        \"name\": \"benchmark_id\",\n+                        \"in\": \"path\",\n+                        \"description\": \"The ID of the benchmark to unregister.\",\n+                        \"required\": true,\n+                        \"schema\": {\n+                            \"type\": \"string\"\n+                        }\n+                    }\n+                ]\n             }\n         },\n         \"/v1/openai/v1/chat/completions/{completion_id}\": {\n@@ -1620,6 +1654,40 @@\n                         }\n                     }\n                 ]\n+            },\n+            \"delete\": {\n+                \"responses\": {\n+                    \"200\": {\n+                        \"description\": \"OK\"\n+                    },\n+                    \"400\": {\n+                        \"$ref\": \"#/components/responses/BadRequest400\"\n+                    },\n+                    \"429\": {\n+                        \"$ref\": \"#/components/responses/TooManyRequests429\"\n+                    },\n+                    \"500\": {\n+                        \"$ref\": \"#/components/responses/InternalServerError500\"\n+                    },\n+                    \"default\": {\n+                        \"$ref\": \"#/components/responses/DefaultError\"\n+                    }\n+                },\n+                \"tags\": [\n+                    \"ScoringFunctions\"\n+                ],\n+                \"description\": \"Unregister a scoring function.\",\n+                \"parameters\": [\n+                    {\n+                        \"name\": \"scoring_fn_id\",\n+                        \"in\": \"path\",\n+                        \"description\": \"The ID of the scoring function to unregister.\",\n+                        \"required\": true,\n+                        \"schema\": {\n+                            \"type\": \"string\"\n+                        }\n+                    }\n+                ]\n             }\n         },\n         \"/v1/shields/{identifier}\": {\ndiff --git a/docs/_static/llama-stack-spec.yaml b/docs/_static/llama-stack-spec.yaml\nindex 8ed04c1f89..94dc5c0f9f 100644\n--- a/docs/_static/llama-stack-spec.yaml\n+++ b/docs/_static/llama-stack-spec.yaml\n@@ -954,6 +954,30 @@ paths:\n           required: true\n           schema:\n             type: string\n+    delete:\n+      responses:\n+        '200':\n+          description: OK\n+        '400':\n+          $ref: '#/components/responses/BadRequest400'\n+        '429':\n+          $ref: >-\n+            #/components/responses/TooManyRequests429\n+        '500':\n+          $ref: >-\n+            #/components/responses/InternalServerError500\n+        default:\n+          $ref: '#/components/responses/DefaultError'\n+      tags:\n+        - Benchmarks\n+      description: Unregister a benchmark.\n+      parameters:\n+        - name: benchmark_id\n+          in: path\n+          description: The ID of the benchmark to unregister.\n+          required: true\n+          schema:\n+            type: string\n   /v1/openai/v1/chat/completions/{completion_id}:\n     get:\n       responses:\n@@ -1119,6 +1143,31 @@ paths:\n           required: true\n           schema:\n             type: string\n+    delete:\n+      responses:\n+        '200':\n+          description: OK\n+        '400':\n+          $ref: '#/components/responses/BadRequest400'\n+        '429':\n+          $ref: >-\n+            #/components/responses/TooManyRequests429\n+        '500':\n+          $ref: >-\n+            #/components/responses/InternalServerError500\n+        default:\n+          $ref: '#/components/responses/DefaultError'\n+      tags:\n+        - ScoringFunctions\n+      description: Unregister a scoring function.\n+      parameters:\n+        - name: scoring_fn_id\n+          in: path\n+          description: >-\n+            The ID of the scoring function to unregister.\n+          required: true\n+          schema:\n+            type: string\n   /v1/shields/{identifier}:\n     get:\n       responses:\ndiff --git a/llama_stack/apis/benchmarks/benchmarks.py b/llama_stack/apis/benchmarks/benchmarks.py\nindex 706eaed6cc..8d0a25e7bd 100644\n--- a/llama_stack/apis/benchmarks/benchmarks.py\n+++ b/llama_stack/apis/benchmarks/benchmarks.py\n@@ -93,3 +93,11 @@ async def register_benchmark(\n         :param metadata: The metadata to use for the benchmark.\n         \"\"\"\n         ...\n+\n+    @webmethod(route=\"/eval/benchmarks/{benchmark_id}\", method=\"DELETE\")\n+    async def unregister_benchmark(self, benchmark_id: str) -> None:\n+        \"\"\"Unregister a benchmark.\n+\n+        :param benchmark_id: The ID of the benchmark to unregister.\n+        \"\"\"\n+        ...\ndiff --git a/llama_stack/apis/scoring_functions/scoring_functions.py b/llama_stack/apis/scoring_functions/scoring_functions.py\nindex 05b6325b74..5410677666 100644\n--- a/llama_stack/apis/scoring_functions/scoring_functions.py\n+++ b/llama_stack/apis/scoring_functions/scoring_functions.py\n@@ -197,3 +197,11 @@ async def register_scoring_function(\n         :param params: The parameters for the scoring function for benchmark eval, these can be overridden for app eval.\n         \"\"\"\n         ...\n+\n+    @webmethod(route=\"/scoring-functions/{scoring_fn_id:path}\", method=\"DELETE\")\n+    async def unregister_scoring_function(self, scoring_fn_id: str) -> None:\n+        \"\"\"Unregister a scoring function.\n+\n+        :param scoring_fn_id: The ID of the scoring function to unregister.\n+        \"\"\"\n+        ...\ndiff --git a/llama_stack/core/routing_tables/benchmarks.py b/llama_stack/core/routing_tables/benchmarks.py\nindex c875dee5b4..8c87d395de 100644\n--- a/llama_stack/core/routing_tables/benchmarks.py\n+++ b/llama_stack/core/routing_tables/benchmarks.py\n@@ -56,3 +56,7 @@ async def register_benchmark(\n             provider_resource_id=provider_benchmark_id,\n         )\n         await self.register_object(benchmark)\n+\n+    async def unregister_benchmark(self, benchmark_id: str) -> None:\n+        existing_benchmark = await self.get_benchmark(benchmark_id)\n+        await self.unregister_object(existing_benchmark)\ndiff --git a/llama_stack/core/routing_tables/common.py b/llama_stack/core/routing_tables/common.py\nindex e523746d8a..ca2f3af42e 100644\n--- a/llama_stack/core/routing_tables/common.py\n+++ b/llama_stack/core/routing_tables/common.py\n@@ -64,6 +64,10 @@ async def unregister_object_from_provider(obj: RoutableObject, p: Any) -> None:\n         return await p.unregister_shield(obj.identifier)\n     elif api == Api.datasetio:\n         return await p.unregister_dataset(obj.identifier)\n+    elif api == Api.eval:\n+        return await p.unregister_benchmark(obj.identifier)\n+    elif api == Api.scoring:\n+        return await p.unregister_scoring_function(obj.identifier)\n     elif api == Api.tool_runtime:\n         return await p.unregister_toolgroup(obj.identifier)\n     else:\ndiff --git a/llama_stack/core/routing_tables/scoring_functions.py b/llama_stack/core/routing_tables/scoring_functions.py\nindex 71e5bed63c..520f070143 100644\n--- a/llama_stack/core/routing_tables/scoring_functions.py\n+++ b/llama_stack/core/routing_tables/scoring_functions.py\n@@ -60,3 +60,7 @@ async def register_scoring_function(\n         )\n         scoring_fn.provider_id = provider_id\n         await self.register_object(scoring_fn)\n+\n+    async def unregister_scoring_function(self, scoring_fn_id: str) -> None:\n+        existing_scoring_fn = await self.get_scoring_function(scoring_fn_id)\n+        await self.unregister_object(existing_scoring_fn)\ndiff --git a/llama_stack/providers/inline/eval/meta_reference/eval.py b/llama_stack/providers/inline/eval/meta_reference/eval.py\nindex 9ae2018c4f..a03e8951ce 100644\n--- a/llama_stack/providers/inline/eval/meta_reference/eval.py\n+++ b/llama_stack/providers/inline/eval/meta_reference/eval.py\n@@ -75,6 +75,13 @@ async def register_benchmark(self, task_def: Benchmark) -> None:\n         )\n         self.benchmarks[task_def.identifier] = task_def\n \n+    async def unregister_benchmark(self, benchmark_id: str) -> None:\n+        if benchmark_id in self.benchmarks:\n+            del self.benchmarks[benchmark_id]\n+\n+        key = f\"{EVAL_TASKS_PREFIX}{benchmark_id}\"\n+        await self.kvstore.delete(key)\n+\n     async def run_eval(\n         self,\n         benchmark_id: str,\ndiff --git a/llama_stack/providers/inline/scoring/llm_as_judge/scoring.py b/llama_stack/providers/inline/scoring/llm_as_judge/scoring.py\nindex fd651877c4..9b76285242 100644\n--- a/llama_stack/providers/inline/scoring/llm_as_judge/scoring.py\n+++ b/llama_stack/providers/inline/scoring/llm_as_judge/scoring.py\n@@ -63,6 +63,9 @@ async def list_scoring_functions(self) -> list[ScoringFn]:\n     async def register_scoring_function(self, function_def: ScoringFn) -> None:\n         self.llm_as_judge_fn.register_scoring_fn_def(function_def)\n \n+    async def unregister_scoring_function(self, scoring_fn_id: str) -> None:\n+        self.llm_as_judge_fn.unregister_scoring_fn_def(scoring_fn_id)\n+\n     async def score_batch(\n         self,\n         dataset_id: str,\ndiff --git a/llama_stack/providers/remote/eval/nvidia/eval.py b/llama_stack/providers/remote/eval/nvidia/eval.py\nindex 3572de0ef6..a474e78e37 100644\n--- a/llama_stack/providers/remote/eval/nvidia/eval.py\n+++ b/llama_stack/providers/remote/eval/nvidia/eval.py\n@@ -51,18 +51,23 @@ async def initialize(self) -> None: ...\n \n     async def shutdown(self) -> None: ...\n \n-    async def _evaluator_get(self, path):\n+    async def _evaluator_get(self, path: str):\n         \"\"\"Helper for making GET requests to the evaluator service.\"\"\"\n         response = requests.get(url=f\"{self.config.evaluator_url}{path}\")\n         response.raise_for_status()\n         return response.json()\n \n-    async def _evaluator_post(self, path, data):\n+    async def _evaluator_post(self, path: str, data: dict[str, Any]):\n         \"\"\"Helper for making POST requests to the evaluator service.\"\"\"\n         response = requests.post(url=f\"{self.config.evaluator_url}{path}\", json=data)\n         response.raise_for_status()\n         return response.json()\n \n+    async def _evaluator_delete(self, path: str) -> None:\n+        \"\"\"Helper for making DELETE requests to the evaluator service.\"\"\"\n+        response = requests.delete(url=f\"{self.config.evaluator_url}{path}\")\n+        response.raise_for_status()\n+\n     async def register_benchmark(self, task_def: Benchmark) -> None:\n         \"\"\"Register a benchmark as an evaluation configuration.\"\"\"\n         await self._evaluator_post(\n@@ -75,6 +80,10 @@ async def register_benchmark(self, task_def: Benchmark) -> None:\n             },\n         )\n \n+    async def unregister_benchmark(self, benchmark_id: str) -> None:\n+        \"\"\"Unregister a benchmark evaluation configuration from NeMo Evaluator.\"\"\"\n+        await self._evaluator_delete(f\"/v1/evaluation/configs/{DEFAULT_NAMESPACE}/{benchmark_id}\")\n+\n     async def run_eval(\n         self,\n         benchmark_id: str,\n",
    "repo": "llamastack/llama-stack",
    "base_commit": "b6cb8178976b941a1fdb3894b00bd13eaca91561",
    "hints_text": "Can I work on that?\n\nCC: @ashwinb ",
    "created_at": "2025-09-08T13:36:31Z",
    "test_patch": "diff --git a/tests/integration/scoring/test_scoring.py b/tests/integration/scoring/test_scoring.py\nindex 315ff050cb..1112f91649 100644\n--- a/tests/integration/scoring/test_scoring.py\n+++ b/tests/integration/scoring/test_scoring.py\n@@ -9,6 +9,7 @@\n \n import pandas as pd\n import pytest\n+import requests\n \n \n @pytest.fixture\n@@ -77,7 +78,46 @@ def test_scoring_functions_register(\n     assert len(list_response) > 0\n     assert any(x.identifier == sample_scoring_fn_id for x in list_response)\n \n-    # TODO: add unregister api for scoring functions\n+\n+def test_scoring_functions_unregister(\n+    llama_stack_client,\n+    sample_scoring_fn_id,\n+    judge_model_id,\n+    sample_judge_prompt_template,\n+):\n+    llm_as_judge_provider = [\n+        x\n+        for x in llama_stack_client.providers.list()\n+        if x.api == \"scoring\" and x.provider_type == \"inline::llm-as-judge\"\n+    ]\n+    if len(llm_as_judge_provider) == 0:\n+        pytest.skip(\"No llm-as-judge provider found, cannot test unregister\")\n+\n+    llm_as_judge_provider_id = llm_as_judge_provider[0].provider_id\n+\n+    # Register first\n+    register_scoring_function(\n+        llama_stack_client,\n+        llm_as_judge_provider_id,\n+        sample_scoring_fn_id,\n+        judge_model_id,\n+        sample_judge_prompt_template,\n+    )\n+\n+    # Ensure it is present\n+    list_response = llama_stack_client.scoring_functions.list()\n+    assert any(x.identifier == sample_scoring_fn_id for x in list_response)\n+\n+    # Unregister scoring fn\n+    try:\n+        base_url = llama_stack_client.base_url\n+    except AttributeError:\n+        pytest.skip(\"No server base_url available; cannot test HTTP unregister in library mode\")\n+\n+    resp = requests.delete(f\"{base_url}/v1/scoring-functions/{sample_scoring_fn_id}\", timeout=30)\n+    assert resp.status_code in (200, 204)\n+    list_after = llama_stack_client.scoring_functions.list()\n+    assert all(x.identifier != sample_scoring_fn_id for x in list_after)\n \n \n @pytest.mark.parametrize(\"scoring_fn_id\", [\"basic::equality\"])\ndiff --git a/tests/unit/distribution/routers/test_routing_tables.py b/tests/unit/distribution/routers/test_routing_tables.py\nindex 1ceee81c6b..bbfea3f466 100644\n--- a/tests/unit/distribution/routers/test_routing_tables.py\n+++ b/tests/unit/distribution/routers/test_routing_tables.py\n@@ -105,6 +105,9 @@ async def list_scoring_functions(self):\n     async def register_scoring_function(self, scoring_fn):\n         return scoring_fn\n \n+    async def unregister_scoring_function(self, scoring_fn_id: str):\n+        return scoring_fn_id\n+\n \n class BenchmarksImpl(Impl):\n     def __init__(self):\n@@ -113,6 +116,9 @@ def __init__(self):\n     async def register_benchmark(self, benchmark):\n         return benchmark\n \n+    async def unregister_benchmark(self, benchmark_id: str):\n+        return benchmark_id\n+\n \n class ToolGroupsImpl(Impl):\n     def __init__(self):\n@@ -330,6 +336,13 @@ async def test_scoring_functions_routing_table(cached_disk_dist_registry):\n     assert \"test-scoring-fn\" in scoring_fn_ids\n     assert \"test-scoring-fn-2\" in scoring_fn_ids\n \n+    # Unregister scoring functions and verify listing\n+    for i in range(len(scoring_functions.data)):\n+        await table.unregister_scoring_function(scoring_functions.data[i].scoring_fn_id)\n+\n+    scoring_functions_list_after_deletion = await table.list_scoring_functions()\n+    assert len(scoring_functions_list_after_deletion.data) == 0\n+\n \n async def test_benchmarks_routing_table(cached_disk_dist_registry):\n     table = BenchmarksRoutingTable({\"test_provider\": BenchmarksImpl()}, cached_disk_dist_registry, {})\n@@ -347,6 +360,15 @@ async def test_benchmarks_routing_table(cached_disk_dist_registry):\n     benchmark_ids = {b.identifier for b in benchmarks.data}\n     assert \"test-benchmark\" in benchmark_ids\n \n+    # Unregister the benchmark and verify removal\n+    await table.unregister_benchmark(benchmark_id=\"test-benchmark\")\n+    benchmarks_after = await table.list_benchmarks()\n+    assert len(benchmarks_after.data) == 0\n+\n+    # Unregistering a non-existent benchmark should raise a clear error\n+    with pytest.raises(ValueError, match=\"Benchmark 'dummy_benchmark' not found\"):\n+        await table.unregister_benchmark(benchmark_id=\"dummy_benchmark\")\n+\n \n async def test_tool_groups_routing_table(cached_disk_dist_registry):\n     table = ToolGroupsRoutingTable({\"test_provider\": ToolGroupsImpl()}, cached_disk_dist_registry, {})\ndiff --git a/tests/unit/providers/nvidia/test_eval.py b/tests/unit/providers/nvidia/test_eval.py\nindex 584ca2101a..2bdcbbebad 100644\n--- a/tests/unit/providers/nvidia/test_eval.py\n+++ b/tests/unit/providers/nvidia/test_eval.py\n@@ -52,14 +52,19 @@ def setUp(self):\n         self.evaluator_post_patcher = patch(\n             \"llama_stack.providers.remote.eval.nvidia.eval.NVIDIAEvalImpl._evaluator_post\"\n         )\n+        self.evaluator_delete_patcher = patch(\n+            \"llama_stack.providers.remote.eval.nvidia.eval.NVIDIAEvalImpl._evaluator_delete\"\n+        )\n \n         self.mock_evaluator_get = self.evaluator_get_patcher.start()\n         self.mock_evaluator_post = self.evaluator_post_patcher.start()\n+        self.mock_evaluator_delete = self.evaluator_delete_patcher.start()\n \n     def tearDown(self):\n         \"\"\"Clean up after each test.\"\"\"\n         self.evaluator_get_patcher.stop()\n         self.evaluator_post_patcher.stop()\n+        self.evaluator_delete_patcher.stop()\n \n     def _assert_request_body(self, expected_json):\n         \"\"\"Helper method to verify request body in Evaluator POST request is correct\"\"\"\n@@ -115,6 +120,13 @@ def test_register_benchmark(self):\n         self.mock_evaluator_post.assert_called_once()\n         self._assert_request_body({\"namespace\": benchmark.provider_id, \"name\": benchmark.identifier, **eval_config})\n \n+    def test_unregister_benchmark(self):\n+        # Unregister the benchmark\n+        self.run_async(self.eval_impl.unregister_benchmark(benchmark_id=MOCK_BENCHMARK_ID))\n+\n+        # Verify the Evaluator API was called correctly\n+        self.mock_evaluator_delete.assert_called_once_with(f\"/v1/evaluation/configs/nvidia/{MOCK_BENCHMARK_ID}\")\n+\n     def test_run_eval(self):\n         benchmark_config = BenchmarkConfig(\n             eval_candidate=ModelCandidate(\n",
    "problem_statement": "feat: create HTTP DELETE API endpoints to allow users to free up ScoringFn and Benchmark resources\n### ðŸš€ Describe the new functionality needed\n\nCurrently, in Llama Stack there is no HTTP DELETE API endpoints for:\n1.  `/eval/benchmarks` (https://github.com/meta-llama/llama-stack/blob/main/llama_stack/apis/benchmarks/benchmarks.py) \n2. `/scoring-functions/{scoring_fn_id:path} `(https://github.com/meta-llama/llama-stack/blob/main/llama_stack/apis/scoring_functions/scoring_functions.py) \n\nHence users won't be able to free up the ScoringFn and Benchmark resources. \n\n### ðŸ’¡ Why is this needed? What if we don't build it?\n\nIf not implement this feature,  users won't be able to free up the ScoringFn and Benchmark resource hence will not fully control the system and manage it. So, this should be done.\n\n### Other thoughts\n\n_No response_\n",
    "environment_setup_commit": "b6cb8178976b941a1fdb3894b00bd13eaca91561",
    "pr_url": "https://github.com/llamastack/llama-stack/pull/3371",
    "issue_url": "https://github.com/llamastack/llama-stack/issues/3051",
    "issue_numbers": [
        "3051"
    ]
}