{
    "instance_id": "llamastack__llama-stack-3386",
    "patch": "diff --git a/llama_stack/providers/remote/inference/bedrock/bedrock.py b/llama_stack/providers/remote/inference/bedrock/bedrock.py\nindex 63ea196f64..106caed9bb 100644\n--- a/llama_stack/providers/remote/inference/bedrock/bedrock.py\n+++ b/llama_stack/providers/remote/inference/bedrock/bedrock.py\n@@ -53,6 +53,43 @@\n \n from .models import MODEL_ENTRIES\n \n+REGION_PREFIX_MAP = {\n+    \"us\": \"us.\",\n+    \"eu\": \"eu.\",\n+    \"ap\": \"ap.\",\n+}\n+\n+\n+def _get_region_prefix(region: str | None) -> str:\n+    # AWS requires region prefixes for inference profiles\n+    if region is None:\n+        return \"us.\"  # default to US when we don't know\n+\n+    # Handle case insensitive region matching\n+    region_lower = region.lower()\n+    for prefix in REGION_PREFIX_MAP:\n+        if region_lower.startswith(f\"{prefix}-\"):\n+            return REGION_PREFIX_MAP[prefix]\n+\n+    # Fallback to US for anything we don't recognize\n+    return \"us.\"\n+\n+\n+def _to_inference_profile_id(model_id: str, region: str = None) -> str:\n+    # Return ARNs unchanged\n+    if model_id.startswith(\"arn:\"):\n+        return model_id\n+\n+    # Return inference profile IDs that already have regional prefixes\n+    if any(model_id.startswith(p) for p in REGION_PREFIX_MAP.values()):\n+        return model_id\n+\n+    # Default to US East when no region is provided\n+    if region is None:\n+        region = \"us-east-1\"\n+\n+    return _get_region_prefix(region) + model_id\n+\n \n class BedrockInferenceAdapter(\n     ModelRegistryHelper,\n@@ -166,8 +203,13 @@ async def _get_params_for_chat_completion(self, request: ChatCompletionRequest)\n             options[\"repetition_penalty\"] = sampling_params.repetition_penalty\n \n         prompt = await chat_completion_request_to_prompt(request, self.get_llama_model(request.model))\n+\n+        # Convert foundation model ID to inference profile ID\n+        region_name = self.client.meta.region_name\n+        inference_profile_id = _to_inference_profile_id(bedrock_model, region_name)\n+\n         return {\n-            \"modelId\": bedrock_model,\n+            \"modelId\": inference_profile_id,\n             \"body\": json.dumps(\n                 {\n                     \"prompt\": prompt,\n@@ -185,6 +227,11 @@ async def embeddings(\n         task_type: EmbeddingTaskType | None = None,\n     ) -> EmbeddingsResponse:\n         model = await self.model_store.get_model(model_id)\n+\n+        # Convert foundation model ID to inference profile ID\n+        region_name = self.client.meta.region_name\n+        inference_profile_id = _to_inference_profile_id(model.provider_resource_id, region_name)\n+\n         embeddings = []\n         for content in contents:\n             assert not content_has_media(content), \"Bedrock does not support media for embeddings\"\n@@ -193,7 +240,7 @@ async def embeddings(\n             body = json.dumps(input_body)\n             response = self.client.invoke_model(\n                 body=body,\n-                modelId=model.provider_resource_id,\n+                modelId=inference_profile_id,\n                 accept=\"application/json\",\n                 contentType=\"application/json\",\n             )\n",
    "repo": "llamastack/llama-stack",
    "base_commit": "167143131053c8de6ea620a83ebdec41c0b24e50",
    "hints_text": "",
    "created_at": "2025-09-09T13:49:49Z",
    "test_patch": "diff --git a/tests/unit/providers/test_bedrock.py b/tests/unit/providers/test_bedrock.py\nnew file mode 100644\nindex 0000000000..1ff07bbbe8\n--- /dev/null\n+++ b/tests/unit/providers/test_bedrock.py\n@@ -0,0 +1,53 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# the root directory of this source tree.\n+\n+from llama_stack.providers.remote.inference.bedrock.bedrock import (\n+    _get_region_prefix,\n+    _to_inference_profile_id,\n+)\n+\n+\n+def test_region_prefixes():\n+    assert _get_region_prefix(\"us-east-1\") == \"us.\"\n+    assert _get_region_prefix(\"eu-west-1\") == \"eu.\"\n+    assert _get_region_prefix(\"ap-south-1\") == \"ap.\"\n+    assert _get_region_prefix(\"ca-central-1\") == \"us.\"\n+\n+    # Test case insensitive\n+    assert _get_region_prefix(\"US-EAST-1\") == \"us.\"\n+    assert _get_region_prefix(\"EU-WEST-1\") == \"eu.\"\n+    assert _get_region_prefix(\"Ap-South-1\") == \"ap.\"\n+\n+    # Test None region\n+    assert _get_region_prefix(None) == \"us.\"\n+\n+\n+def test_model_id_conversion():\n+    # Basic conversion\n+    assert (\n+        _to_inference_profile_id(\"meta.llama3-1-70b-instruct-v1:0\", \"us-east-1\") == \"us.meta.llama3-1-70b-instruct-v1:0\"\n+    )\n+\n+    # Already has prefix\n+    assert (\n+        _to_inference_profile_id(\"us.meta.llama3-1-70b-instruct-v1:0\", \"us-east-1\")\n+        == \"us.meta.llama3-1-70b-instruct-v1:0\"\n+    )\n+\n+    # ARN should be returned unchanged\n+    arn = \"arn:aws:bedrock:us-east-1:123456789012:inference-profile/us.meta.llama3-1-70b-instruct-v1:0\"\n+    assert _to_inference_profile_id(arn, \"us-east-1\") == arn\n+\n+    # ARN should be returned unchanged even without region\n+    assert _to_inference_profile_id(arn) == arn\n+\n+    # Optional region parameter defaults to us-east-1\n+    assert _to_inference_profile_id(\"meta.llama3-1-70b-instruct-v1:0\") == \"us.meta.llama3-1-70b-instruct-v1:0\"\n+\n+    # Different regions work with optional parameter\n+    assert (\n+        _to_inference_profile_id(\"meta.llama3-1-70b-instruct-v1:0\", \"eu-west-1\") == \"eu.meta.llama3-1-70b-instruct-v1:0\"\n+    )\n",
    "problem_statement": "Bedrock provider fails due to AWS requiring Inference Profile IDs instead of foundation model IDs\n### System Info\n\n```\n aws bedrock-runtime invoke-model --model-id \"**us.**meta.llama3-1-8b-instruct-v1:0\" \\\n    --body '{\"prompt\":\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nYour\n  question<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\"max_gen_len\":100}' \\\n    --cli-binary-format raw-in-base64-out response.json\n```\nneed to add inference_id instead of current model_id\n\n\n### üêõ Describe the bug\n\n The Bedrock provider is seems to be broken due to AWS changing their requirements for on-demand throughput. AWS now requires using Inference Profile IDs instead of direct foundation model IDs, causing all inference calls to fail.\n\n Root Cause:\n\n  - We use foundation model IDs like meta.llama3-1-70b-instruct-v1:0\n  - AWS now requires inference profile IDs like **us**.meta.llama3-1-70b-instruct-v1:0\n  - When trying to register inference profile IDs, LlamaStack's validation rejects them as unsupported models\n\nProposed Solutions - Please give Feedback\n\n  Option 1: Inference Profile Config \n\n  config:\n    inference_profile_id: \"us.meta.llama3-1-70b-instruct-v1:0\"\n  Call with this instead of model_id.\n\n  Option 2: Model Mapping via Environment Variable\n\n  config:\n    model_mapping_json: ${env.BEDROCK_MODEL_MAPPINGS:={}}\n  export BEDROCK_MODEL_MAPPINGS='{\"meta.llama3-1-70b-instruct-v1:0\": \"us.meta.llama3-1-70b-instruct-v1:0\"}'\n\n   Option 3: Auto-detection based on AWS region\n\n  Dynamically prefix based on detected AWS region from boto3 (us./eu./ap.) \n\nPlease suggest if there are other options that we can apply. Thanks!\n\n\n### Error logs\n\n```\n  ValidationException: Invocation of model ID meta.llama3-1-70b-instruct-v1:0 with on-demand throughput isn't  supported. Retry your request with the ID or ARN of an inference profile that contains this model.\n```\n\n### Expected behavior\n\nInference calls get a response no above error.\n",
    "environment_setup_commit": "167143131053c8de6ea620a83ebdec41c0b24e50",
    "pr_url": "https://github.com/llamastack/llama-stack/pull/3386",
    "issue_url": "https://github.com/llamastack/llama-stack/issues/3370",
    "issue_numbers": [
        "3370"
    ]
}